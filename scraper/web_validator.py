#!/usr/bin/env python3
"""
ç¶²ç«™è³‡æ–™é©—è­‰å™¨ - é©ç”¨æ–¼ CXMS åŠå…¶ä»–ç¶²ç«™ä¾†æº
åŠŸèƒ½ï¼š
1. åçˆ¬èŸ²åµæ¸¬ï¼ˆrobots.txtã€Cloudflareã€JS æ¸²æŸ“éœ€æ±‚ï¼‰
2. çˆ¬å–æ™‚åŒæ­¥å»ºç«‹å¿«ç…§ï¼ˆHTML + æˆªåœ– + æå–çµæœï¼‰
3. å¿«ç…§å­˜æ–¼ snapshots/web/{clinic_id}/

ç”¨æ³•ï¼š
  # åªåšåçˆ¬èŸ²åµæ¸¬
  python3 web_validator.py --check-only

  # å°å–®ä¸€è¨ºæ‰€æˆªåœ–å¿«ç…§
  python3 web_validator.py --clinic c02

  # å°æ‰€æœ‰ CXMS è¨ºæ‰€åšå¿«ç…§
  python3 web_validator.py --all-cxms

  # å°æŒ‡å®š URL åšå¿«ç…§ï¼ˆä¸éœ€è¦ clinic_idï¼‰
  python3 web_validator.py --url http://web.cxms.com.tw/wn/hosp.php --clinic c02
"""

import argparse
import json
import time
import urllib.request
import urllib.robotparser
from datetime import datetime
from pathlib import Path

SCRAPER_DIR = Path(__file__).parent
SNAPSHOT_DIR = SCRAPER_DIR / "snapshots" / "web"
SCHEDULES_JSON = SCRAPER_DIR.parent / "schedules.json"

# CXMS è¨ºæ‰€æ¸…å–®ï¼ˆå¾ schedules.json è®€å–ï¼‰
CXMS_CLINICS = {}

def load_cxms_clinics():
    """å¾ schedules.json è®€å– CXMS è¨ºæ‰€"""
    global CXMS_CLINICS
    with open(SCHEDULES_JSON, encoding="utf-8") as f:
        data = json.load(f)
    for c in data["clinics"]:
        url = c.get("source_url", "")
        if "cxms.com.tw" in url:
            CXMS_CLINICS[c["id"]] = {
                "name": c["name"],
                "url": url,
            }
    return CXMS_CLINICS


def check_robots_txt(base_url: str) -> dict:
    """æª¢æŸ¥ robots.txt æ˜¯å¦å…è¨±çˆ¬å–"""
    result = {"allowed": True, "robots_url": "", "error": None}
    try:
        # å–å¾— base URL
        from urllib.parse import urlparse
        parsed = urlparse(base_url)
        robots_url = f"{parsed.scheme}://{parsed.netloc}/robots.txt"
        result["robots_url"] = robots_url

        rp = urllib.robotparser.RobotFileParser()
        rp.set_url(robots_url)
        rp.read()
        result["allowed"] = rp.can_fetch("*", base_url)
    except Exception as e:
        result["error"] = str(e)
        result["allowed"] = True  # ç„¡æ³•è®€å–æ™‚é è¨­å…è¨±
    return result


def check_anti_scraping(url: str, clinic_name: str = "") -> dict:
    """
    åµæ¸¬åçˆ¬èŸ²æ©Ÿåˆ¶
    å›å‚³ï¼š
      - status_code: HTTP ç‹€æ…‹ç¢¼
      - has_cloudflare: æ˜¯å¦æœ‰ Cloudflare
      - needs_js: æ˜¯å¦éœ€è¦ JS æ¸²æŸ“ï¼ˆé é¢å…§å®¹æ¥µå°‘ï¼‰
      - robots_ok: robots.txt æ˜¯å¦å…è¨±
      - response_size: å›æ‡‰å¤§å°ï¼ˆbytesï¼‰
      - accessible: æ˜¯å¦å¯å­˜å–
    """
    result = {
        "clinic": clinic_name,
        "url": url,
        "status_code": None,
        "has_cloudflare": False,
        "needs_js": False,
        "robots_ok": True,
        "response_size": 0,
        "accessible": False,
        "error": None,
        "checked_at": datetime.now().isoformat(),
    }

    # 1. æª¢æŸ¥ robots.txt
    robots = check_robots_txt(url)
    result["robots_ok"] = robots["allowed"]
    if not robots["allowed"]:
        print(f"  âš ï¸  robots.txt ä¸å…è¨±çˆ¬å–: {url}")

    # 2. ç™¼é€ HTTP è«‹æ±‚
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
                          "AppleWebKit/537.36 (KHTML, like Gecko) "
                          "Chrome/121.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "zh-TW,zh;q=0.9,en;q=0.8",
        }
        req = urllib.request.Request(url, headers=headers)
        with urllib.request.urlopen(req, timeout=15) as resp:
            result["status_code"] = resp.status
            content = resp.read()
            result["response_size"] = len(content)
            html = content.decode("utf-8", errors="replace")

            # åµæ¸¬ Cloudflare
            if any(kw in html for kw in ["cloudflare", "cf-ray", "__cf_bm", "Checking your browser"]):
                result["has_cloudflare"] = True

            # åµæ¸¬æ˜¯å¦éœ€è¦ JSï¼ˆé é¢å…§å®¹æ¥µå°‘ï¼‰
            if len(html) < 500:
                result["needs_js"] = True
            elif "<table" not in html.lower() and "<div" not in html.lower():
                result["needs_js"] = True

            result["accessible"] = True
            result["_html_preview"] = html[:300]  # å‰ 300 å­—å…ƒä¾›é™¤éŒ¯

    except urllib.error.HTTPError as e:
        result["status_code"] = e.code
        result["error"] = f"HTTP {e.code}: {e.reason}"
    except Exception as e:
        result["error"] = str(e)

    return result


def scrape_with_snapshot(url: str, clinic_id: str, clinic_name: str = "") -> dict:
    """
    çˆ¬å–ç¶²é ä¸¦å»ºç«‹å¿«ç…§
    å¿«ç…§å…§å®¹ï¼š
      - {date}_html.html     å®Œæ•´ HTML
      - {date}_meta.json     çˆ¬å– metadataï¼ˆURLã€æ™‚é–“ã€ç‹€æ…‹ï¼‰
    æ³¨æ„ï¼šæˆªåœ–éœ€è¦ Seleniumï¼Œæ­¤å‡½æ•¸å…ˆåš HTML å¿«ç…§
    """
    date_str = datetime.now().strftime("%Y%m%d_%H%M%S")
    clinic_dir = SNAPSHOT_DIR / clinic_id
    clinic_dir.mkdir(parents=True, exist_ok=True)

    result = {
        "clinic_id": clinic_id,
        "clinic_name": clinic_name,
        "url": url,
        "snapshot_dir": str(clinic_dir),
        "date_str": date_str,
        "html_file": None,
        "meta_file": None,
        "success": False,
        "error": None,
    }

    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
                          "AppleWebKit/537.36 (KHTML, like Gecko) "
                          "Chrome/121.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "zh-TW,zh;q=0.9,en;q=0.8",
        }
        req = urllib.request.Request(url, headers=headers)
        with urllib.request.urlopen(req, timeout=15) as resp:
            content = resp.read()
            html = content.decode("utf-8", errors="replace")

        # å„²å­˜ HTML å¿«ç…§
        html_file = clinic_dir / f"{date_str}_html.html"
        html_file.write_text(html, encoding="utf-8")
        result["html_file"] = str(html_file)

        # å„²å­˜ metadata
        meta = {
            "clinic_id": clinic_id,
            "clinic_name": clinic_name,
            "url": url,
            "scraped_at": datetime.now().isoformat(),
            "html_file": str(html_file),
            "html_size_bytes": len(content),
            "status": "success",
        }
        meta_file = clinic_dir / f"{date_str}_meta.json"
        meta_file.write_text(json.dumps(meta, ensure_ascii=False, indent=2), encoding="utf-8")
        result["meta_file"] = str(meta_file)
        result["success"] = True

        print(f"  âœ… {clinic_name}: HTML å¿«ç…§å·²å„²å­˜ ({len(content):,} bytes)")
        print(f"     â†’ {html_file}")

    except Exception as e:
        result["error"] = str(e)
        print(f"  âŒ {clinic_name}: çˆ¬å–å¤±æ•— - {e}")

    return result


def run_check_only():
    """åªåšåçˆ¬èŸ²åµæ¸¬ï¼Œä¸å„²å­˜å¿«ç…§"""
    clinics = load_cxms_clinics()
    print(f"\nğŸ” åçˆ¬èŸ²åµæ¸¬ - {len(clinics)} å€‹ CXMS è¨ºæ‰€\n")
    print(f"{'è¨ºæ‰€':12} {'ç‹€æ…‹ç¢¼':8} {'CF':5} {'éœ€JS':6} {'robots':7} {'å¤§å°':10} {'èªªæ˜'}")
    print("-" * 70)

    results = []
    for clinic_id, info in clinics.items():
        print(f"  æª¢æŸ¥ {info['name']} ({info['url']})...")
        r = check_anti_scraping(info["url"], info["name"])
        results.append(r)

        status = r.get("status_code", "ERR")
        cf = "âš ï¸" if r["has_cloudflare"] else "âœ…"
        js = "âš ï¸" if r["needs_js"] else "âœ…"
        robots = "âœ…" if r["robots_ok"] else "âš ï¸"
        size = f"{r['response_size']:,}" if r["response_size"] else "â€”"
        note = r.get("error", "OK") or "OK"

        print(f"  {info['name']:10} {str(status):8} {cf:5} {js:6} {robots:7} {size:10} {note[:30]}")
        time.sleep(1)  # é¿å…éå¿«è«‹æ±‚

    # å„²å­˜åµæ¸¬çµæœ
    report_file = SCRAPER_DIR / f"anti_scraping_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(report_file, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    print(f"\nğŸ“„ å ±å‘Šå·²å„²å­˜: {report_file}")

    # æ‘˜è¦
    accessible = sum(1 for r in results if r["accessible"])
    has_cf = sum(1 for r in results if r["has_cloudflare"])
    needs_js = sum(1 for r in results if r["needs_js"])
    print(f"\nğŸ“Š æ‘˜è¦: {accessible}/{len(results)} å¯å­˜å–, {has_cf} æœ‰CF, {needs_js} éœ€JSæ¸²æŸ“")

    return results


def run_snapshot(clinic_id: str = None, url: str = None):
    """å°æŒ‡å®šè¨ºæ‰€æˆ– URL åšå¿«ç…§"""
    clinics = load_cxms_clinics()

    if clinic_id and clinic_id in clinics:
        info = clinics[clinic_id]
        result = scrape_with_snapshot(info["url"], clinic_id, info["name"])
    elif url and clinic_id:
        result = scrape_with_snapshot(url, clinic_id, clinic_id)
    else:
        print("âŒ è«‹æŒ‡å®š --clinic æˆ–åŒæ™‚æŒ‡å®š --url å’Œ --clinic")
        return

    return result


def run_all_cxms_snapshots():
    """å°æ‰€æœ‰ CXMS è¨ºæ‰€åšå¿«ç…§"""
    clinics = load_cxms_clinics()
    print(f"\nğŸ“¸ é–‹å§‹å° {len(clinics)} å€‹ CXMS è¨ºæ‰€å»ºç«‹å¿«ç…§...\n")
    results = []
    for clinic_id, info in clinics.items():
        r = scrape_with_snapshot(info["url"], clinic_id, info["name"])
        results.append(r)
        time.sleep(2)  # é¿å…éå¿«è«‹æ±‚

    success = sum(1 for r in results if r["success"])
    print(f"\nâœ¨ å®Œæˆ: {success}/{len(results)} æˆåŠŸ")
    return results


def main():
    parser = argparse.ArgumentParser(description="ç¶²ç«™è³‡æ–™é©—è­‰å™¨")
    parser.add_argument("--check-only", action="store_true", help="åªåšåçˆ¬èŸ²åµæ¸¬")
    parser.add_argument("--clinic", help="è¨ºæ‰€ ID (ä¾‹å¦‚ c02)")
    parser.add_argument("--url", help="æŒ‡å®š URL")
    parser.add_argument("--all-cxms", action="store_true", help="å°æ‰€æœ‰ CXMS è¨ºæ‰€åšå¿«ç…§")
    args = parser.parse_args()

    if args.check_only:
        run_check_only()
    elif args.all_cxms:
        run_all_cxms_snapshots()
    elif args.clinic or args.url:
        run_snapshot(clinic_id=args.clinic, url=args.url)
    else:
        # é è¨­ï¼šå…ˆåšåµæ¸¬
        print("æœªæŒ‡å®šæ¨¡å¼ï¼ŒåŸ·è¡Œåçˆ¬èŸ²åµæ¸¬...")
        run_check_only()


if __name__ == "__main__":
    main()
